{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVpEzTC+SoZ3A4XKEgjpz1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rachit180/Machine_fault_detection/blob/main/Machine_fault_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGDiEkvMMGz6"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/drive/MyDrive/id_00.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_MQGG90MScF"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow tensorflow-io matplotliba"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7SiptW25w7pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "from glob import glob\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "from itertools import cycle\n",
        "\n",
        "sns.set_theme(style='white',palette=None)\n",
        "color_pal = plt.rcParams['axes.prop_cycle'].by_key()[\"color\"]\n",
        "color_cycle = cycle(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])\n",
        "Bh"
      ],
      "metadata": {
        "id": "cAGyyGc7w9fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_audio_files = \"/content/id_00/normal\"\n",
        "abnormal_audio_files = \"/content/id_00/abnormal\""
      ],
      "metadata": {
        "id": "puBLSmjCw_sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "\n",
        "normal_audio_dir = \"/content/id_00/normal\"\n",
        "abnormal_audio_dir = \"/content/id_00/abnormal\"\n",
        "\n",
        "num_files_to_select = 600\n",
        "\n",
        "# Get a list of all normal audio files\n",
        "normal_files = [filename for filename in os.listdir(normal_audio_dir) if filename.endswith(\".wav\")]\n",
        "\n",
        "# Randomly select 600 normal audio files\n",
        "selected_normal_files = random.sample(normal_files, num_files_to_select)"
      ],
      "metadata": {
        "id": "gAtaT8avxDxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#time-frequency analysis\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from scipy.signal import stft\n",
        "\n",
        "normal_stft = []\n",
        "abnormal_stft = []\n",
        "\n",
        "# Process normal audio files\n",
        "for filename in selected_normal_files:\n",
        "    filepath = os.path.join(normal_audio_dir, filename)\n",
        "    audio, sr = librosa.load(filepath, sr=None)\n",
        "\n",
        "    # Apply Short-Time Fourier Transform to obtain the time-frequency analysis\n",
        "    _, _, Zxx = stft(audio, fs=sr)\n",
        "\n",
        "    normal_stft.append(np.abs(Zxx))\n",
        "\n",
        "# Process abnormal audio files\n",
        "for filename in os.listdir(abnormal_audio_dir):\n",
        "    if filename.endswith(\".wav\"):\n",
        "        filepath = os.path.join(abnormal_audio_dir, filename)\n",
        "        audio, sr = librosa.load(filepath, sr=None)\n",
        "\n",
        "        # Apply Short-Time Fourier Transform to obtain the time-frequency representation\n",
        "        _, _, Zxx = stft(audio, fs=sr)\n",
        "\n",
        "        abnormal_stft.append(np.abs(Zxx))"
      ],
      "metadata": {
        "id": "Pos1ZJs5xECa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_stft = np.array(normal_stft)\n",
        "abnormal_stft = np.array(abnormal_stft)"
      ],
      "metadata": {
        "id": "RUxAqmkMxEIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_stft = np.reshape(normal_stft, (normal_stft.shape[0], -1))\n",
        "abnormal_stft = np.reshape(abnormal_stft, (abnormal_stft.shape[0], -1))"
      ],
      "metadata": {
        "id": "QoKae0cWxEM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(normal_stft.shape)"
      ],
      "metadata": {
        "id": "tG2aCotvxEQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mel spectrogram\n",
        "normal_specs = []\n",
        "abnormal_specs = []\n",
        "\n",
        "for audio in normal_stft:\n",
        "  # Compute Mel spectrogram\n",
        "\n",
        "  normal_specs.append(mel_spectrogram)\n",
        "\n",
        "for audio in abnormal_stft:\n",
        "  mel_spectrogram = librosa.feature.melspectrogram(y=audio,sr=sr)\n",
        "  abnormal_specs.append(mel_spectrogram)"
      ],
      "metadata": {
        "id": "3weVwX5zxETo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_specs = np.array(normal_specs)\n",
        "abnormal_specs = np.array(abnormal_specs)"
      ],
      "metadata": {
        "id": "yHW8nuaDxEWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "for mel_spec in normal_specs[:1]:\n",
        "    librosa.display.specshow(librosa.power_to_db(mel_spec, ref=np.max),\n",
        "                             y_axis='mel', x_axis='time')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title('Mel spectrogram (Normal)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot Mel spectrograms for abnormal audio\n",
        "plt.figure(figsize=(10, 6))\n",
        "for mel_spec in abnormal_specs[:1]:\n",
        "    librosa.display.specshow(librosa.power_to_db(mel_spec, ref=np.max),\n",
        "                             y_axis='mel', x_axis='time')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title('Mel spectrogram (Abnormal)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QRiMDjhcxO3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(normal_specs.shape)\n",
        "print(abnormal_specs.shape)"
      ],
      "metadata": {
        "id": "kpBnfeWHxO6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''#Mel spectrogram\n",
        "normal_specs = []\n",
        "abnormal_specs = []\n",
        "\n",
        "# Process selected normal audio files\n",
        "for filename in selected_normal_files:\n",
        "    filepath = os.path.join(normal_audio_dir, filename)\n",
        "    audio, sr = librosa.load(filepath, sr=None)\n",
        "\n",
        "    # Compute Mel spectrogram\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(y=audio,sr=sr)\n",
        "    normal_specs.append(mel_spectrogram)\n",
        "\n",
        "# Process abnormal audio files\n",
        "for filename in os.listdir(abnormal_audio_dir):\n",
        "    if filename.endswith(\".wav\"):\n",
        "        filepath = os.path.join(abnormal_audio_dir, filename)\n",
        "        audio, sr = librosa.load(filepath, sr=None)\n",
        "\n",
        "    # Compute Mel spectrogram\n",
        "        mel_spectrogram = librosa.feature.melspectrogram(y=audio,sr=sr)\n",
        "        abnormal_specs.append(mel_spectrogram)\n",
        "\n",
        "normal_specs = np.array(normal_specs)\n",
        "abnormal_specs = np.array(abnormal_specs)'''"
      ],
      "metadata": {
        "id": "I6N0pZIdxO9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_specs = np.reshape(normal_specs, (normal_specs.shape[0], -1))\n",
        "abnormal_specs = np.reshape(abnormal_specs, (abnormal_specs.shape[0], -1))"
      ],
      "metadata": {
        "id": "MFRqgVu4xPAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(normal_specs.shape)\n",
        "print(abnormal_specs.shape)"
      ],
      "metadata": {
        "id": "CtKUl5xixPDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MFCC\n",
        "normal_mfcc = []\n",
        "abnormal_mfcc = []\n",
        "\n",
        "for audio in normal_stft:\n",
        "  mfcc = librosa.feature.mfcc(y=audio, sr=sr)\n",
        "  normal_mfcc.append(mfcc)\n",
        "\n",
        "for audio in abnormal_stft:\n",
        "  mfcc = librosa.feature.mfcc(y=audio, sr=sr)\n",
        "  abnormal_mfcc.append(mfcc)"
      ],
      "metadata": {
        "id": "MgLZSPY-xPGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_mfcc = np.array(normal_mfcc)\n",
        "abnormal_mfcc = np.array(abnormal_mfcc)"
      ],
      "metadata": {
        "id": "SOFRxX5Rxat4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_mfcc = np.reshape(normal_mfcc, (normal_mfcc.shape[0], -1))\n",
        "abnormal_mfcc = np.reshape(abnormal_mfcc, (abnormal_mfcc.shape[0], -1))"
      ],
      "metadata": {
        "id": "qlHaWrQxxaw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(normal_mfcc.shape)\n",
        "print(abnormal_mfcc.shape)"
      ],
      "metadata": {
        "id": "yBOYvZdVxazz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''#MFCC\n",
        "normal_mfcc = []\n",
        "abnormal_mfcc=[]\n",
        "\n",
        "# Process selected normal audio files\n",
        "for filename in selected_normal_files:\n",
        "    filepath = os.path.join(normal_audio_dir, filename)\n",
        "    audio, sr = librosa.load(filepath, sr=None)\n",
        "\n",
        "    # Compute MFCC\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sr)\n",
        "    normal_mfcc.append(mfcc)\n",
        "\n",
        "\n",
        "# Process abnormal audio files\n",
        "for filename in os.listdir(abnormal_audio_dir):\n",
        "    if filename.endswith(\".wav\"):\n",
        "        filepath = os.path.join(abnormal_audio_dir, filename)\n",
        "        audio, sr = librosa.load(filepath, sr=None)\n",
        "\n",
        "    # Compute MFCC\n",
        "        mfcc = librosa.feature.mfcc(y=audio, sr=sr)\n",
        "        abnormal_mfcc.append(mfcc)\n",
        "\n",
        "normal_mfcc = np.array(normal_mfcc)\n",
        "abnormal_mfcc = np.array(abnormal_mfcc)'''"
      ],
      "metadata": {
        "id": "E539Mx7Gxa2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(normal_mfcc.shape)"
      ],
      "metadata": {
        "id": "WcvkygOmxa5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spectral kurtosis\n",
        "\n",
        "from scipy.stats import kurtosis\n",
        "\n",
        "normal_kurtosis = []\n",
        "abnormal_kurtosis = []\n",
        "\n",
        "for i in range(normal_stft.shape[0]):\n",
        "  S = librosa.stft(normal_stft[i])\n",
        "  sk = kurtosis(np.abs(S),axis=1)\n",
        "\n",
        "  normal_kurtosis.append(sk)\n",
        "\n",
        "for i in range(abnormal_stft.shape[0]):\n",
        "\n",
        "  S = librosa.stft(abnormal_stft[i])\n",
        "  sk = kurtosis(np.abs(S), axis=1)\n",
        "\n",
        "  abnormal_kurtosis.append(sk)\n",
        "\n",
        "normal_kurtosis = np.array(normal_kurtosis)\n",
        "abnormal_kurtosis = np.array(abnormal_kurtosis)"
      ],
      "metadata": {
        "id": "f_J3LmL1xa76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "for sk_vals in normal_kurtosis[:1]:\n",
        "    plt.plot(sk_vals, label='Normal')\n",
        "plt.ylabel('Spectral Kurtosis')\n",
        "plt.title('Spectral Kurtosis (Normal)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot Spectral Kurtosis for abnormal audio\n",
        "plt.figure(figsize=(10, 6))\n",
        "for sk_vals in abnormal_kurtosis[:1]:\n",
        "    plt.plot(sk_vals, label='Abnormal')\n",
        "plt.ylabel('Spectral Kurtosis')\n",
        "plt.title('Spectral Kurtosis (Abnormal)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_KJQsgvsxpE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(normal_kurtosis.shape)\n",
        "print(abnormal_kurtosis.shape)"
      ],
      "metadata": {
        "id": "3RdQE4V4xpHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Spectral centroid\n",
        "\n",
        "normal_centroid = []\n",
        "abnormal_centroid = []\n",
        "\n",
        "for i in range(normal_stft.shape[0]):\n",
        "  S = librosa.stft(normal_stft[i])\n",
        "  centroid = librosa.feature.spectral_centroid(S=np.abs(S))\n",
        "\n",
        "  normal_centroid.append(centroid)\n",
        "\n",
        "for i in range(abnormal_stft.shape[0]):\n",
        "  S = librosa.stft(abnormal_stft[i])\n",
        "  centroid = librosa.feature.spectral_centroid(S=np.abs(S))\n",
        "\n",
        "  abnormal_centroid.append(centroid)"
      ],
      "metadata": {
        "id": "jqpYUOxsxpKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_centroid = np.array(normal_centroid)\n",
        "abnormal_centroid = np.array(abnormal_centroid)"
      ],
      "metadata": {
        "id": "faZuXYMVxpN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "for centroid_vals in normal_centroid[:1]:\n",
        "    plt.semilogy(centroid_vals.T, label='Normal')\n",
        "plt.ylabel('Spectral Centroid')\n",
        "plt.title('Spectral Centroid (Normal)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot Spectral Centroid for abnormal audio\n",
        "plt.figure(figsize=(10, 6))\n",
        "for centroid_vals in abnormal_centroid[:1]:\n",
        "    plt.semilogy(centroid_vals.T, label='Abnormal')\n",
        "plt.ylabel('Spectral Centroid')\n",
        "plt.title('Spectral Centroid (Abnormal)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gMcQzYIixpQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(normal_centroid.shape)\n",
        "print(abnormal_centroid.shape)"
      ],
      "metadata": {
        "id": "xPSH5iL_xpTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_centroid = np.reshape(normal_centroid, (normal_centroid.shape[0], -1))\n",
        "abnormal_centroid = np.reshape(abnormal_centroid, (abnormal_centroid.shape[0], -1))\n",
        "print(normal_centroid.shape)\n",
        "print(abnormal_centroid.shape)"
      ],
      "metadata": {
        "id": "uQh7o_DhxpWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(normal_centroid.shape)"
      ],
      "metadata": {
        "id": "ZmlaEvLJxpZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Zero crossing rate\n",
        "\n",
        "normal_zcr=[]\n",
        "abnormal_zcr=[]\n",
        "\n",
        "for i in range(normal_stft.shape[0]):\n",
        "  zcr = librosa.feature.zero_crossing_rate(normal_stft[i])\n",
        "  normal_zcr.append(zcr)\n",
        "\n",
        "for i in range(abnormal_stft.shape[0]):\n",
        "  zcr = librosa.feature.zero_crossing_rate(abnormal_stft[i])\n",
        "  abnormal_zcr.append(zcr)"
      ],
      "metadata": {
        "id": "K2CN3Iy9xpb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_zcr = np.array(normal_zcr)\n",
        "abnormal_zcr = np.array(abnormal_zcr)\n"
      ],
      "metadata": {
        "id": "sgRcsEMFxpew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(normal_zcr.shape)\n",
        "print(abnormal_zcr.shape)"
      ],
      "metadata": {
        "id": "ifQtLusOxphu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_zcr = np.reshape(normal_zcr, (normal_zcr.shape[0], -1))\n",
        "abnormal_zcr = np.reshape(abnormal_zcr, (abnormal_zcr.shape[0], -1))\n",
        "print(normal_zcr.shape)\n",
        "print(abnormal_zcr.shape)"
      ],
      "metadata": {
        "id": "cZePLfsWxpks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_contac_all = np.concatenate((normal_specs,normal_kurtosis,normal_centroid),axis=1)\n",
        "abnormal_contac_all = np.concatenate((abnormal_specs,abnormal_kurtosis,abnormal_centroid),axis=1)"
      ],
      "metadata": {
        "id": "6gVJYG5nxpn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(normal_contac_all.shape)\n",
        "print(abnormal_contac_all.shape)"
      ],
      "metadata": {
        "id": "wNINEPM7xpqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(normal_contac_all.shape)\n",
        "print(abnormal_contac_all.shape)"
      ],
      "metadata": {
        "id": "ZmueI6OhxpuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y1=np.zeros(normal_contac_all.shape[0])\n",
        "y2=np.ones(abnormal_contac_all.shape[0])\n",
        "y_features=np.concatenate((y1, y2))\n",
        "# print(y_train)\n",
        "\n",
        "X_features=np.concatenate((normal_contac_all,abnormal_contac_all ))\n",
        "print(y_features.shape)\n",
        "print(X_features.shape)"
      ],
      "metadata": {
        "id": "TW5KZxJXyel_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a NumPy array\n",
        "data = X_features\n",
        "labels=y_features"
      ],
      "metadata": {
        "id": "JoUGKSn_yepI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create a NumPy array of labels\n",
        "# labels = np.array([0, 1, 0, 1])\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Shuffle the indices\n",
        "indices = np.random.permutation(len(data))\n",
        "\n",
        "# Shuffle the data and labels based on the shuffled indices\n",
        "shuffled_data = data[indices]\n",
        "shuffled_labels = labels[indices]\n",
        "\n",
        "# Set the desired ratio for train-test split\n",
        "train_ratio = 0.75\n",
        "\n",
        "# Calculate the split index\n",
        "split_index = int(train_ratio * len(data))\n",
        "\n",
        "# Split the data and labels into train and test sets\n",
        "train_data_features = shuffled_data[:split_index]\n",
        "train_labels_features = shuffled_labels[:split_index]\n",
        "test_data_features = shuffled_data[split_index:]\n",
        "test_labels_features = shuffled_labels[split_index:]\n",
        "\n",
        "# Print the train and test sets\n",
        "print(\"Training Data:\")\n",
        "print(train_data_features)\n",
        "print(\"Training Labels:\")\n",
        "print(train_labels_features)\n",
        "print(\"Testing Data:\")\n",
        "print(test_data_features)\n",
        "print(\"Testing Labels:\")\n",
        "print(test_labels_features)"
      ],
      "metadata": {
        "id": "KvfZrpWAyesR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "#np.random.seed(42)\n",
        "\n",
        "# Split the data and labels into train and test sets\n",
        "train_data_features, test_data_features, train_labels_features, test_labels_features = train_test_split(\n",
        "    data, labels, train_size=0.75, random_state=0)\n",
        "\n",
        "# Print the train and test sets\n",
        "print(\"Training Data:\")\n",
        "print(train_data_features)\n",
        "print(\"Training Labels:\")\n",
        "print(train_labels_features)\n",
        "print(\"Testing Data:\")\n",
        "print(test_data_features)\n",
        "print(\"Testing Labels:\")\n",
        "print(test_labels_features)\n"
      ],
      "metadata": {
        "id": "t0wwEzeMyevX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_data = train_data_features\n",
        "train_labels = train_labels_features\n",
        "#test_data = test_data_features\n",
        "test_labels = test_labels_features"
      ],
      "metadata": {
        "id": "0mPv_Hk4yeyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_data = scaler.fit_transform(train_data_features)\n",
        "test_data = scaler.transform(test_data_features)"
      ],
      "metadata": {
        "id": "XVE3nq9nye1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ensemble model of Random forest and XGboost\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "random_forest = RandomForestClassifier()\n",
        "xgboost = XGBClassifier()\n",
        "\n",
        "ensemble_model = VotingClassifier(estimators=[('rf', random_forest), ('xgb', xgboost)], voting='hard')\n",
        "ensemble_model.fit(train_data, train_labels)\n",
        "ensemble_model.predict(test_data)\n",
        "ensemble_model.score(test_data, test_labels)\n"
      ],
      "metadata": {
        "id": "8u6NSRTCyuhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ensemble_model.predict(test_data)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(test_labels, y_pred)\n",
        "\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "id": "GiGdOR2hyulQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ensemble model of RFC and SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "random_forest = RandomForestClassifier()\n",
        "svc = SVC(probability=True)  # Set probability=True for soft voting\n",
        "\n",
        "ensemble_model = VotingClassifier(estimators=[('rf', random_forest), ('svc', svc)], voting='soft')\n",
        "\n",
        "ensemble_model.fit(train_data, train_labels)\n",
        "ensemble_model.predict(test_data)\n",
        "ensemble_model.score(test_data, test_labels)\n"
      ],
      "metadata": {
        "id": "ZByy0g9-yuoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ensemble_model.predict(test_data)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(test_labels, y_pred)\n",
        "\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "id": "08sybb4Yyuqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ensemble model of SVC and XGboost\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "svc = SVC(probability=True)  # Set probability=True for soft voting\n",
        "xgboost = XGBClassifier()\n",
        "\n",
        "ensemble_model = VotingClassifier(estimators=[('svc', svc), ('xgb', xgboost)], voting='soft')\n",
        "\n",
        "ensemble_model.fit(train_data, train_labels)\n",
        "ensemble_model.predict(test_data)\n",
        "ensemble_model.score(test_data, test_labels)\n"
      ],
      "metadata": {
        "id": "mNeMsVCtyutV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Assuming you have already trained and tested the ensemble_model\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ensemble_model.predict(test_data)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(test_labels, y_pred)\n",
        "\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "id": "FGGBvvPly2NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# # Generate dummy data\n",
        "# data = np.random.random((1000, 10))  # Replace with your own data\n",
        "# labels = np.random.randint(2, size=(1000, 1))  # Replace with your own labels\n",
        "\n",
        "# Split data into training and testing sets\n",
        "#train_data = train_data_features\n",
        "train_labels = train_labels_features\n",
        "#test_data = test_data_features\n",
        "test_labels = test_labels_features\n",
        "\n",
        "# Build the ANN model\n",
        "model = Sequential()\n",
        "model.add(Dense(256, input_dim=train_data.shape[1], activation='relu'))\n",
        "# model.add(Dropout(0.2))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "# model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# Set the learning rate\n",
        "# learning_rate = 0.01\n",
        "# optimizer = Adam(learning_rate=learning_rate)\n",
        "# Train the model\n",
        "model.fit(train_data, train_labels, epochs=50, batch_size=16, verbose=1)\n",
        "# model.fit(train_data, train_labels, verbose=1)\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_data, test_labels, verbose=1)\n",
        "print('Test Loss:', loss)\n",
        "print('Test Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "id": "JDrc-A1Ly2Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Assuming you have the predictions for the test set\n",
        "predictions = model.predict(test_data)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(test_labels, predictions)\n",
        "\n",
        "print('F1 Score:', f1)\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "# Assuming you have the predictions for the test set\n",
        "predictions = model.predict(test_data)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(test_labels, predictions)\n",
        "\n",
        "print('F1 Score:', f1)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(test_labels, predictions)\n",
        "print('Confusion Matrix:')\n",
        "print(cm)\n"
      ],
      "metadata": {
        "id": "eE2wbH0Sy2TX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# # Generate dummy data\n",
        "# data = np.random.random((1000, 10))  # Replace with your own data\n",
        "# labels = np.random.randint(2, size=(1000, 1))  # Replace with your own labels\n",
        "\n",
        "# # Split data into training and testing sets\n",
        "train_data = train_data_features\n",
        "train_labels = train_labels_features\n",
        "test_data = test_data_features\n",
        "test_labels = test_labels_features\n",
        "\n",
        "# Reshape data to fit LSTM input shape\n",
        "train_data = np.reshape(train_data, (train_data.shape[0], 1, train_data.shape[1]))\n",
        "test_data = np.reshape(test_data, (test_data.shape[0], 1, test_data.shape[1]))\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(1, train_data.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_data, train_labels, epochs=25, batch_size=16, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_data, test_labels, verbose=1)\n",
        "print('Test Loss:', loss)\n",
        "print('Test Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "id": "6OFerhGTy2V_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Assuming you have the predictions for the test set\n",
        "predictions = model.predict(test_data)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(test_labels, predictions)\n",
        "\n",
        "print('F1 Score:', f1)\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "# Assuming you have the predictions for the test set\n",
        "predictions = model.predict(test_data)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(test_labels, predictions)\n",
        "\n",
        "print('F1 Score:', f1)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(test_labels, predictions)\n",
        "print('Confusion Matrix:')\n",
        "print(cm)\n"
      ],
      "metadata": {
        "id": "0PJZRqSDy2Y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split data into training and testing sets\n",
        "#train_data = train_data_features\n",
        "train_labels = train_labels_features\n",
        "#test_data = test_data_features\n",
        "test_labels = test_labels_features\n",
        "\n",
        "# Create a Random Forest classifier\n",
        "model = RandomForestClassifier(n_estimators=60)\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_data, train_labels)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.predict(test_data)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "print('Test Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "id": "oOI6Keq1y2bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "# Assuming you have the predictions for the test set\n",
        "predictions = model.predict(test_data)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(test_labels, predictions)\n",
        "\n",
        "print('F1 Score:', f1)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(test_labels, predictions)\n",
        "print('Confusion Matrix:')\n",
        "print(cm)\n",
        "\n"
      ],
      "metadata": {
        "id": "MUEk0mrWy2eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming you have the training and testing data and labels\n",
        "# train_data = train_data_features\n",
        "train_labels = train_labels_features\n",
        "# test_data = test_data_features\n",
        "test_labels = test_labels_features\n",
        "\n",
        "# Create the XGBoost Classifier model\n",
        "model = xgb.XGBClassifier()\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_data, train_labels)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.predict(test_data)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "print('Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "id": "tzXHZO-5zFhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Assuming you have the predictions for the test set\n",
        "predictions = model.predict(test_data)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(test_labels, predictions)\n",
        "\n",
        "print('F1 Score:', f1)\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(test_labels, predictions)\n",
        "print('Confusion Matrix:')\n",
        "print(cm)\n"
      ],
      "metadata": {
        "id": "ugx2sKOmzFj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yj4Dm3VOzFrG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}